{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 128, Minibatch Loss= 1.097922, Training Accuracy= 0.45312\n",
      "Step 25600, Minibatch Loss= 0.687614, Training Accuracy= 0.53846\n",
      "Step 51200, Minibatch Loss= 0.685480, Training Accuracy= 0.50962\n",
      "Step 76800, Minibatch Loss= 0.682016, Training Accuracy= 0.55769\n",
      "Step 102400, Minibatch Loss= 0.677113, Training Accuracy= 0.55769\n",
      "Step 128000, Minibatch Loss= 0.669381, Training Accuracy= 0.63462\n",
      "Step 153600, Minibatch Loss= 0.655567, Training Accuracy= 0.65385\n",
      "Step 179200, Minibatch Loss= 0.626822, Training Accuracy= 0.74038\n",
      "Step 204800, Minibatch Loss= 0.568278, Training Accuracy= 0.74038\n",
      "Step 230400, Minibatch Loss= 0.509938, Training Accuracy= 0.73077\n",
      "Step 256000, Minibatch Loss= 0.484085, Training Accuracy= 0.75962\n",
      "Step 281600, Minibatch Loss= 0.474520, Training Accuracy= 0.78846\n",
      "Step 307200, Minibatch Loss= 0.468898, Training Accuracy= 0.77885\n",
      "Step 332800, Minibatch Loss= 0.464187, Training Accuracy= 0.76923\n",
      "Step 358400, Minibatch Loss= 0.459615, Training Accuracy= 0.76923\n",
      "Step 384000, Minibatch Loss= 0.455026, Training Accuracy= 0.76923\n",
      "Step 409600, Minibatch Loss= 0.450433, Training Accuracy= 0.78846\n",
      "Step 435200, Minibatch Loss= 0.445874, Training Accuracy= 0.78846\n",
      "Step 460800, Minibatch Loss= 0.441364, Training Accuracy= 0.78846\n",
      "Step 486400, Minibatch Loss= 0.436859, Training Accuracy= 0.78846\n",
      "Step 512000, Minibatch Loss= 0.432233, Training Accuracy= 0.79808\n",
      "Step 537600, Minibatch Loss= 0.427287, Training Accuracy= 0.79808\n",
      "Step 563200, Minibatch Loss= 0.421750, Training Accuracy= 0.79808\n",
      "Step 588800, Minibatch Loss= 0.415241, Training Accuracy= 0.79808\n",
      "Step 614400, Minibatch Loss= 0.407153, Training Accuracy= 0.80769\n",
      "Step 640000, Minibatch Loss= 0.396496, Training Accuracy= 0.80769\n",
      "Step 665600, Minibatch Loss= 0.381735, Training Accuracy= 0.83654\n",
      "Step 691200, Minibatch Loss= 0.359247, Training Accuracy= 0.84615\n",
      "Step 716800, Minibatch Loss= 0.327453, Training Accuracy= 0.88462\n",
      "Step 742400, Minibatch Loss= 0.295873, Training Accuracy= 0.92308\n",
      "Step 768000, Minibatch Loss= 0.263011, Training Accuracy= 0.95192\n",
      "Step 793600, Minibatch Loss= 0.228810, Training Accuracy= 0.99038\n",
      "Step 819200, Minibatch Loss= 0.193449, Training Accuracy= 1.00000\n",
      "Step 844800, Minibatch Loss= 0.157871, Training Accuracy= 1.00000\n",
      "Step 870400, Minibatch Loss= 0.132333, Training Accuracy= 1.00000\n",
      "Step 896000, Minibatch Loss= 0.111894, Training Accuracy= 1.00000\n",
      "Step 921600, Minibatch Loss= 0.114058, Training Accuracy= 1.00000\n",
      "Step 947200, Minibatch Loss= 0.094919, Training Accuracy= 1.00000\n",
      "Step 972800, Minibatch Loss= 0.087224, Training Accuracy= 1.00000\n",
      "Step 998400, Minibatch Loss= 0.081299, Training Accuracy= 1.00000\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Dynamic Recurrent Neural Network.\n",
    "TensorFlow implementation of a Recurrent Neural Network (LSTM) that performs\n",
    "dynamic computation over sequences with variable length. This example is using\n",
    "a toy dataset to classify linear sequences. The generated sequences have\n",
    "variable length.\n",
    "Links:\n",
    "    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "\n",
    "# ====================\n",
    "#  TOY DATA GENERATOR\n",
    "# ====================\n",
    "class ToySequenceData(object):\n",
    "    \"\"\" Generate sequence of data with dynamic length.\n",
    "    This class generate samples for training:\n",
    "    - Class 0: linear sequences (i.e. [0, 1, 2, 3,...])\n",
    "    - Class 1: random sequences (i.e. [1, 3, 10, 7,...])\n",
    "    NOTICE:\n",
    "    We have to pad each sequence to reach 'max_seq_len' for TensorFlow\n",
    "    consistency (we cannot feed a numpy array with inconsistent\n",
    "    dimensions). The dynamic calculation will then be perform thanks to\n",
    "    'seqlen' attribute that records every actual sequence length.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_samples=1000, max_seq_len=20, min_seq_len=3,\n",
    "                 max_value=1000):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.seqlen = []\n",
    "        for i in range(n_samples):\n",
    "            # Random sequence length\n",
    "            len = random.randint(min_seq_len, max_seq_len)\n",
    "            # Monitor sequence length for TensorFlow dynamic calculation\n",
    "            self.seqlen.append(len)\n",
    "            # Add a random or linear int sequence (50% prob)\n",
    "            if random.random() < .5:\n",
    "                # Generate a linear sequence\n",
    "                rand_start = random.randint(0, max_value - len)\n",
    "                s = [[float(i)/max_value] for i in\n",
    "                     range(rand_start, rand_start + len)]\n",
    "                # Pad sequence for dimension consistency\n",
    "                s += [[0.] for i in range(max_seq_len - len)]\n",
    "                self.data.append(s)\n",
    "                self.labels.append([1., 0.])\n",
    "            else:\n",
    "                # Generate a random sequence\n",
    "                s = [[float(random.randint(0, max_value))/max_value]\n",
    "                     for i in range(len)]\n",
    "                # Pad sequence for dimension consistency\n",
    "                s += [[0.] for i in range(max_seq_len - len)]\n",
    "                self.data.append(s)\n",
    "                self.labels.append([0., 1.])\n",
    "        self.batch_id = 0\n",
    "\n",
    "    def next(self, batch_size):\n",
    "        \"\"\" Return a batch of data. When dataset end is reached, start over.\n",
    "        \"\"\"\n",
    "        if self.batch_id == len(self.data):\n",
    "            self.batch_id = 0\n",
    "        batch_data = (self.data[self.batch_id:min(self.batch_id +\n",
    "                                                  batch_size, len(self.data))])\n",
    "        batch_labels = (self.labels[self.batch_id:min(self.batch_id +\n",
    "                                                  batch_size, len(self.data))])\n",
    "        batch_seqlen = (self.seqlen[self.batch_id:min(self.batch_id +\n",
    "                                                  batch_size, len(self.data))])\n",
    "        self.batch_id = min(self.batch_id + batch_size, len(self.data))\n",
    "        return batch_data, batch_labels, batch_seqlen\n",
    "\n",
    "\n",
    "# ==========\n",
    "#   MODEL\n",
    "# ==========\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_steps = 10000\n",
    "batch_size = 128\n",
    "display_step = 200\n",
    "\n",
    "# Network Parameters\n",
    "seq_max_len = 20 # Sequence max length\n",
    "n_hidden = 64 # hidden layer num of features\n",
    "n_classes = 2 # linear sequence or not\n",
    "\n",
    "trainset = ToySequenceData(n_samples=1000, max_seq_len=seq_max_len)\n",
    "testset = ToySequenceData(n_samples=500, max_seq_len=seq_max_len)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, seq_max_len, 1])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "# A placeholder for indicating each sequence length\n",
    "seqlen = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def dynamicRNN(x, seqlen, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, seq_max_len, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden)\n",
    "\n",
    "    # Get lstm cell output, providing 'sequence_length' will perform dynamic\n",
    "    # calculation.\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32,\n",
    "                                sequence_length=seqlen)\n",
    "\n",
    "    # When performing dynamic calculation, we must retrieve the last\n",
    "    # dynamically computed output, i.e., if a sequence length is 10, we need\n",
    "    # to retrieve the 10th output.\n",
    "    # However TensorFlow doesn't support advanced indexing yet, so we build\n",
    "    # a custom op that for each sample in batch size, get its length and\n",
    "    # get the corresponding relevant output.\n",
    "\n",
    "    # 'outputs' is a list of output at every timestep, we pack them in a Tensor\n",
    "    # and change back dimension to [batch_size, n_step, n_input]\n",
    "    outputs = tf.stack(outputs)\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "\n",
    "    # Hack to build the indexing and retrieve the right output.\n",
    "    batch_size = tf.shape(outputs)[0]\n",
    "    # Start indices for each sample\n",
    "    index = tf.range(0, batch_size) * seq_max_len + (seqlen - 1)\n",
    "    # Indexing\n",
    "    outputs = tf.gather(tf.reshape(outputs, [-1, n_hidden]), index)\n",
    "\n",
    "    # Linear activation, using outputs computed above\n",
    "    return tf.matmul(outputs, weights['out']) + biases['out']\n",
    "\n",
    "pred = dynamicRNN(x, seqlen, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, training_steps + 1):\n",
    "        batch_x, batch_y, batch_seqlen = trainset.next(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n",
    "                                       seqlen: batch_seqlen})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y,\n",
    "                                                seqlen: batch_seqlen})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y,\n",
    "                                             seqlen: batch_seqlen})\n",
    "            print(\"Step \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy\n",
    "    test_data = testset.data\n",
    "    test_label = testset.labels\n",
    "    test_seqlen = testset.seqlen\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y: test_label,\n",
    "seqlen: test_seqlen}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
